{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import array \n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import sklearn.linear_model as lm, pandas as pd, sklearn.ensemble as se, numpy as np\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "from numpy import mean, std\n",
    "from sklearn import svm\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd\n",
    "from numpy import mean, std\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "YEAR = 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data()->pd.DataFrame:\n",
    "    df = pd.read_csv('data/data.csv')\n",
    "    df.drop(columns=['wins', 'losses'], inplace=True)\n",
    "    df['comp_pct'] = df['pass_cmp']/df['pass_att']\n",
    "    df = df[df['year'] > YEAR]\n",
    "    df.columns = df.columns.str.replace('_y', 'y')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prep_season_data()->pd.DataFrame:\n",
    "    df = get_data()\n",
    "    to_not_average = ['year', 'team', 'ties', 'win_loss_perc', 'yds_per_play_offense', 'pass_net_yds_per_att', 'rush_yds_per_att','score_pct', 'turnover_pct','g', \"comp_pct\", \"points_diff\", \"mov\" ]\n",
    "    for col in df.columns:\n",
    "        if col not in to_not_average:\n",
    "            df[col] = df[col]/df['g']\n",
    "    df['mov'] = df['points_diff']/ df['g']\n",
    "    return df.drop(columns=['g'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ties_home has a skew of 3.3982205579576967\n",
      "ties_away has a skew of 3.7748213284572896\n",
      "spread_favorite has a skew of -1.1136612667391173\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>win_loss_perc_home</th>\n",
       "      <th>points_home</th>\n",
       "      <th>points_diff_home</th>\n",
       "      <th>totalyards_home</th>\n",
       "      <th>yds_per_play_offense_home</th>\n",
       "      <th>first_down_home</th>\n",
       "      <th>pass_td_home</th>\n",
       "      <th>pass_netyds_per_att_home</th>\n",
       "      <th>score_pct_home</th>\n",
       "      <th>exp_pts_tot_home</th>\n",
       "      <th>win_loss_perc_away</th>\n",
       "      <th>points_away</th>\n",
       "      <th>points_opp_away</th>\n",
       "      <th>points_diff_away</th>\n",
       "      <th>mov_away</th>\n",
       "      <th>pass_netyds_per_att_away</th>\n",
       "      <th>score_pct_away</th>\n",
       "      <th>exp_pts_tot_away</th>\n",
       "      <th>home_fav</th>\n",
       "      <th>home_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.665722</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.587224</td>\n",
       "      <td>0.495638</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.484277</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.621262</td>\n",
       "      <td>0.515714</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.390663</td>\n",
       "      <td>0.390663</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.392027</td>\n",
       "      <td>0.417663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.665722</td>\n",
       "      <td>0.649660</td>\n",
       "      <td>0.653563</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.635220</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.685983</td>\n",
       "      <td>0.832861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.923833</td>\n",
       "      <td>0.923833</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.784053</td>\n",
       "      <td>0.744285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.165722</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.113022</td>\n",
       "      <td>0.148295</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.242525</td>\n",
       "      <td>0.207251</td>\n",
       "      <td>0.916431</td>\n",
       "      <td>0.935374</td>\n",
       "      <td>0.315353</td>\n",
       "      <td>0.889435</td>\n",
       "      <td>0.889435</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.800664</td>\n",
       "      <td>0.865091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.165722</td>\n",
       "      <td>0.319728</td>\n",
       "      <td>0.194103</td>\n",
       "      <td>0.396114</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.440252</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.491694</td>\n",
       "      <td>0.409302</td>\n",
       "      <td>0.582153</td>\n",
       "      <td>0.435374</td>\n",
       "      <td>0.518672</td>\n",
       "      <td>0.407862</td>\n",
       "      <td>0.407862</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.617940</td>\n",
       "      <td>0.545167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.749292</td>\n",
       "      <td>0.544218</td>\n",
       "      <td>0.584767</td>\n",
       "      <td>0.475813</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.408805</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.421927</td>\n",
       "      <td>0.185577</td>\n",
       "      <td>0.916431</td>\n",
       "      <td>0.840136</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.760797</td>\n",
       "      <td>0.687252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.582153</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.582310</td>\n",
       "      <td>0.540444</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.65625</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.647841</td>\n",
       "      <td>0.729892</td>\n",
       "      <td>0.832861</td>\n",
       "      <td>0.867347</td>\n",
       "      <td>0.087137</td>\n",
       "      <td>0.975430</td>\n",
       "      <td>0.975430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833887</td>\n",
       "      <td>0.896999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.208215</td>\n",
       "      <td>0.180272</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.312450</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.314465</td>\n",
       "      <td>0.18750</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.299003</td>\n",
       "      <td>0.185765</td>\n",
       "      <td>0.832861</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.269710</td>\n",
       "      <td>0.808354</td>\n",
       "      <td>0.808354</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.707641</td>\n",
       "      <td>0.649188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.582153</td>\n",
       "      <td>0.731293</td>\n",
       "      <td>0.712531</td>\n",
       "      <td>0.551546</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.591195</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.679805</td>\n",
       "      <td>0.665722</td>\n",
       "      <td>0.649660</td>\n",
       "      <td>0.365145</td>\n",
       "      <td>0.653563</td>\n",
       "      <td>0.653563</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.685983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.498771</td>\n",
       "      <td>0.657415</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.634551</td>\n",
       "      <td>0.565385</td>\n",
       "      <td>0.124646</td>\n",
       "      <td>0.302721</td>\n",
       "      <td>0.788382</td>\n",
       "      <td>0.152334</td>\n",
       "      <td>0.152334</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.468439</td>\n",
       "      <td>0.429811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.416431</td>\n",
       "      <td>0.336735</td>\n",
       "      <td>0.533170</td>\n",
       "      <td>0.439334</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.554817</td>\n",
       "      <td>0.543066</td>\n",
       "      <td>0.832861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.923833</td>\n",
       "      <td>0.923833</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.784053</td>\n",
       "      <td>0.744285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>550 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     win_loss_perc_home  points_home  points_diff_home  totalyards_home  \\\n",
       "224            0.665722     0.387755          0.587224         0.495638   \n",
       "179            0.665722     0.649660          0.653563         0.744251   \n",
       "113            0.165722     0.074830          0.113022         0.148295   \n",
       "563            0.165722     0.319728          0.194103         0.396114   \n",
       "401            0.749292     0.544218          0.584767         0.475813   \n",
       "..                  ...          ...               ...              ...   \n",
       "501            0.582153     0.500000          0.582310         0.540444   \n",
       "253            0.208215     0.180272          0.162162         0.312450   \n",
       "55             0.582153     0.731293          0.712531         0.551546   \n",
       "123            0.500000     0.642857          0.498771         0.657415   \n",
       "87             0.416431     0.336735          0.533170         0.439334   \n",
       "\n",
       "     yds_per_play_offense_home  first_down_home  pass_td_home  \\\n",
       "224                       0.56         0.484277       0.25000   \n",
       "179                       0.80         0.635220       0.46875   \n",
       "113                       0.24         0.245283       0.12500   \n",
       "563                       0.44         0.440252       0.21875   \n",
       "401                       0.28         0.408805       0.40625   \n",
       "..                         ...              ...           ...   \n",
       "501                       0.60         0.490566       0.65625   \n",
       "253                       0.28         0.314465       0.18750   \n",
       "55                        0.60         0.591195       0.50000   \n",
       "123                       0.64         0.452830       0.71875   \n",
       "87                        0.52         0.446541       0.28125   \n",
       "\n",
       "     pass_netyds_per_att_home  score_pct_home  exp_pts_tot_home  \\\n",
       "224                  0.380952        0.621262          0.515714   \n",
       "179                  0.833333        0.697674          0.685983   \n",
       "113                  0.214286        0.242525          0.207251   \n",
       "563                  0.261905        0.491694          0.409302   \n",
       "401                  0.309524        0.421927          0.185577   \n",
       "..                        ...             ...               ...   \n",
       "501                  0.547619        0.647841          0.729892   \n",
       "253                  0.238095        0.299003          0.185765   \n",
       "55                   0.452381        0.767442          0.679805   \n",
       "123                  0.595238        0.634551          0.565385   \n",
       "87                   0.476190        0.554817          0.543066   \n",
       "\n",
       "     win_loss_perc_away  points_away  points_opp_away  points_diff_away  \\\n",
       "224            0.500000     0.261905         0.336100          0.390663   \n",
       "179            0.832861     1.000000         0.336100          0.923833   \n",
       "113            0.916431     0.935374         0.315353          0.889435   \n",
       "563            0.582153     0.435374         0.518672          0.407862   \n",
       "401            0.916431     0.840136         0.012448          1.000000   \n",
       "..                  ...          ...              ...               ...   \n",
       "501            0.832861     0.867347         0.087137          0.975430   \n",
       "253            0.832861     0.785714         0.269710          0.808354   \n",
       "55             0.665722     0.649660         0.365145          0.653563   \n",
       "123            0.124646     0.302721         0.788382          0.152334   \n",
       "87             0.832861     1.000000         0.336100          0.923833   \n",
       "\n",
       "     mov_away  pass_netyds_per_att_away  score_pct_away  exp_pts_tot_away  \\\n",
       "224  0.390663                  0.404762        0.392027          0.417663   \n",
       "179  0.923833                  0.690476        0.784053          0.744285   \n",
       "113  0.889435                  0.642857        0.800664          0.865091   \n",
       "563  0.407862                  0.523810        0.617940          0.545167   \n",
       "401  1.000000                  0.619048        0.760797          0.687252   \n",
       "..        ...                       ...             ...               ...   \n",
       "501  0.975430                  1.000000        0.833887          0.896999   \n",
       "253  0.808354                  0.523810        0.707641          0.649188   \n",
       "55   0.653563                  0.833333        0.697674          0.685983   \n",
       "123  0.152334                  0.357143        0.468439          0.429811   \n",
       "87   0.923833                  0.690476        0.784053          0.744285   \n",
       "\n",
       "     home_fav  home_win  \n",
       "224       0.0     False  \n",
       "179       0.0     False  \n",
       "113       0.0      True  \n",
       "563       0.0      True  \n",
       "401       0.0     False  \n",
       "..        ...       ...  \n",
       "501       0.0      True  \n",
       "253       0.0      True  \n",
       "55        0.0     False  \n",
       "123       1.0      True  \n",
       "87        0.0     False  \n",
       "\n",
       "[550 rows x 20 columns]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_team_df():\n",
    "    team_df = pd.read_csv('data/nfl_teams.csv')\n",
    "    team_df.drop(columns=[\"team_id_pfr\",\"team_conference_pre2002\", \"team_division_pre2002\" ], inplace=True)\n",
    "    return team_df\n",
    "\n",
    "def get_games_df():\n",
    "    games= pd.read_csv('data/spreadspoke_scores.csv')\n",
    "    games = games[games['schedule_season'] > YEAR]\n",
    "    games = games[games['schedule_week'] != '1']\n",
    "    games = games[games['schedule_week'] != '2']\n",
    "    games = games[games['schedule_week'] !='3']\n",
    "    games = games[games['schedule_week'] !='4']\n",
    "    games = games[games['schedule_week'] !='5']\n",
    "    games = games[games['schedule_week'] !='6']\n",
    "\n",
    "    games['spread_favorite_sort'] = abs(games['spread_favorite'])\n",
    "    \n",
    "    return games\n",
    "    \n",
    "\n",
    "def get_stadiums():\n",
    "    stadiums = pd.read_csv('data/nfl_stadiums.csv')\n",
    "    return stadiums\n",
    "\n",
    "def mege_dfs():\n",
    "    team_df = get_team_df()\n",
    "    df = prep_season_data()\n",
    "    df = df.merge(team_df, left_on='team', right_on='team_name', how='left')\n",
    "    stadiums = get_stadiums()\n",
    "    games_df = get_games_df()\n",
    "    games_df = games_df.merge(stadiums, left_on='stadium', right_on='stadium_name', how='left')\n",
    "    games_df =games_df[games_df['stadium_neutral'] == False]\n",
    "\n",
    "    games_df.drop(columns=['stadium_name', 'stadium_location', 'stadium_open', 'stadium_close', 'stadium_type', 'stadium_address', 'stadium_weather_station_zipcode', 'stadium_surface', 'stadium_weather_station', 'stadium_weather_station_name', 'stadium_latitude', 'stadium_longitude', 'stadium_azimuthangle', 'stadium_elevation', 'weather_temperature', 'weather_wind_mph', 'weather_humidity', 'weather_detail', 'stadium_capacity', \"stadium\"], inplace=True)\n",
    "    games_df = df.merge(games_df, left_on=['year', 'team'], right_on=['schedule_season', 'team_home' ], how='left')\n",
    "    games_df = df.merge(games_df, left_on=['year', 'team'], right_on=['schedule_season', 'team_away' ], how='left')\n",
    "    games_df.columns = games_df.columns.str.replace('_x', '_home')\n",
    "    games_df.columns = games_df.columns.str.replace('_y', '_away')\n",
    "    games_df['home_fav'] = games_df['team_favorite_id'] == games_df['team_id_home']\n",
    "    games_df['home_win'] = games_df['score_home'] > games_df['score_away']\n",
    "    games_df = games_df.sort_values(by='spread_favorite_sort', ascending=True)\n",
    "    games_df['over_under_line'] = games_df['over_under_line'].astype(float)\n",
    "    games_df['over_under_line'] = games_df['over_under_line'].fillna(games_df['over_under_line'].mean())\n",
    "\n",
    "    columns_to_drop = [\n",
    "    'spread_favorite_sort', 'stadium_neutral', 'team_favorite_id', 'team_away', \n",
    "    'schedule_playoff', 'team_home', 'schedule_season', 'schedule_week', \n",
    "    'team_name_away', 'team_name_short_away', 'team_id_away', \n",
    "    'team_conference_away', 'team_division_away', 'schedule_date', \n",
    "    'team_away', 'team_name_home', 'team_name_short_home', 'team_id_home', \n",
    "    'team_conference_home', 'team_division_home', 'year_away', 'year_home','over_under_line', 'score_home', 'score_away'\n",
    "    ]\n",
    "    games_df.drop(columns=columns_to_drop, inplace=True)\n",
    "    return games_df.sample(n=550, random_state=1)\n",
    "\n",
    "def bin_data(df):\n",
    "    df_weather = pd.DataFrame({\"Count\": df['stadium_weather_type'].value_counts()})\n",
    "    df_weather['Proportion'] = df_weather['Count'] / df.shape[0]\n",
    "    #bin moderate and warm together since they make up a smaller portion and warmer weather is not \n",
    "    # a bigger factor in october\n",
    "    df['stadium_weather_type'] = df['stadium_weather_type'].replace(['moderate', 'warm'], 'moderate/warm')\n",
    "    return df\n",
    "\n",
    "\n",
    "def check_skew(df):\n",
    "    cols_to_drop = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            skewness = skew(df[col])\n",
    "            if skewness > 1 or skewness < -1:\n",
    "                print(f'{col} has a skew of {skewness}')\n",
    "                cols_to_drop.append(col)\n",
    "    if len(cols_to_drop) > 0:\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "    return df\n",
    "\n",
    "def missing_data(df):\n",
    "    missing_data = pd.DataFrame({\"Count\": df.isnull().sum()})\n",
    "    missing_data['Proportion'] = missing_data['Count'] / df.shape[0]\n",
    "    missing_data = missing_data[missing_data['Count'] > 0]\n",
    "    for col in missing_data.index:\n",
    "        df[col] = df[col].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def Xandy(df, label):\n",
    "    y = df[label]\n",
    "    X = df.drop(columns=[label])\n",
    "    return X, y\n",
    "\n",
    "def dummy_code(X):\n",
    "    X = pd.get_dummies(X, drop_first=True, dtype=float)\n",
    "    return X\n",
    "    \n",
    "def minmax(X):\n",
    "    X = pd.DataFrame(MinMaxScaler().fit_transform(X.copy()), columns=X.columns, index=X.index)\n",
    "    return X\n",
    "\n",
    "def impute_KNN(df, label, neighbors=5 ):\n",
    "    df = dummy_code(df.copy())\n",
    "    X, y = Xandy(df, label)\n",
    "    X = minmax(X.copy())\n",
    "    imp = KNNImputer(n_neighbors=neighbors, weights=\"uniform\")\n",
    "    X = pd.DataFrame(imp.fit_transform(X), columns=X.columns, index=X.index)\n",
    "    return X.merge(y, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "def select_features(df, label,  max='auto'):\n",
    "\n",
    "    X, y = Xandy(df, label)\n",
    "    clf = ExtraTreesClassifier(n_estimators=100)\n",
    "    clf = clf.fit(X, y)\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    selected_columns = X.columns[model.get_support()]\n",
    "    return df[selected_columns].merge(y, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "df = mege_dfs()\n",
    "df = missing_data(df)\n",
    "df = bin_data(df)\n",
    "df= check_skew(df)\n",
    "df = impute_KNN(df, 'home_win')\n",
    "df.head()\n",
    "df = select_features(df, 'home_win')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Logistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[460], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df_fit)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m models[best_model]\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m---> 57\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfit_cv_classification_expanded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhome_win\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[460], line 45\u001b[0m, in \u001b[0;36mfit_cv_classification_expanded\u001b[0;34m(df, label, k, r, repeat, random_state)\u001b[0m\n\u001b[1;32m     24\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRidge\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeuralN\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layer_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m100\u001b[39m,), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_iter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m200\u001b[39m}\n\u001b[1;32m     42\u001b[0m }\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, hyperparams \u001b[38;5;129;01min\u001b[39;00m hyperparameters\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 45\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(**hyperparams)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Create the model object with specified hyperparameters\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     fit[model_name] \u001b[38;5;241m=\u001b[39m mean(cross_val_score(model, X, y, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39mcv, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     47\u001b[0m     models[model_name] \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Logistic' is not defined"
     ]
    }
   ],
   "source": [
    "def fit_cv_classification_expanded(df, label, k=3, r=10, repeat=True, random_state=1):\n",
    "        X = df.drop(columns=[label])\n",
    "        y = df[label]\n",
    "      \n",
    "        if repeat:\n",
    "          cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=random_state)\n",
    "        else:\n",
    "          cv = KFold(n_splits=k, random_state=random_state, shuffle=True)\n",
    "        \n",
    "        fit = {}    # Use this to store each of the fit metrics\n",
    "        models = {} # Use this to store each of the models\n",
    "        \n",
    "        # Create the model objects\n",
    "        model_log = lm.LogisticRegression(max_iter=100)\n",
    "        model_logcv = lm.RidgeClassifier()\n",
    "        model_sgd = lm.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "        model_pa = lm.PassiveAggressiveClassifier(max_iter=1000, random_state=random_state, tol=1e-3)\n",
    "        model_per = lm.Perceptron(fit_intercept=False, max_iter=10, tol=None, shuffle=False)\n",
    "        model_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "        model_svm = svm.SVC(decision_function_shape='ovo') # Remove the parameter for two-class model\n",
    "        model_nb = CategoricalNB()\n",
    "        model_bag = se.BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "        model_ada = se.AdaBoostClassifier(n_estimators=100, random_state=random_state)\n",
    "        model_ext = se.ExtraTreesClassifier(n_estimators=100, random_state=random_state)\n",
    "        model_rf = se.RandomForestClassifier(n_estimators=10)\n",
    "        model_hgb = se.HistGradientBoostingClassifier(max_iter=100)\n",
    "        model_vot = se.VotingClassifier(estimators=[('lr', model_log), ('rf', model_ext), ('gnb', model_hgb)], voting='hard')\n",
    "        model_gb = se.GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "        estimators = [('ridge', lm.RidgeCV()), ('lasso', lm.LassoCV(random_state=random_state)), ('knr', KNeighborsRegressor(n_neighbors=20, metric='euclidean'))]\n",
    "        final_estimator = se.GradientBoostingRegressor(n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1, random_state=random_state)\n",
    "        model_st = se.StackingRegressor(estimators=estimators, final_estimator=final_estimator)\n",
    "        model_xgb = XGBClassifier( random_state=random_state, use_label_encoder=True, eval_metric='mlogloss')\n",
    "        model_nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=random_state)\n",
    "      \n",
    "        # Fit a cross-validated R squared score and add it to the dict\n",
    "        fit['Logistic'] = mean(cross_val_score(model_log, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['Ridge'] = mean(cross_val_score(model_logcv, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['SGD'] = mean(cross_val_score(model_sgd, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['PassiveAggressive'] = mean(cross_val_score(model_pa, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['Perceptron'] = mean(cross_val_score(model_per, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['KNN'] = mean(cross_val_score(model_knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['SVM'] = mean(cross_val_score(model_svm, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['NaiveBayes'] = mean(cross_val_score(model_nb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['Bagging'] = mean(cross_val_score(model_bag, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['AdaBoost'] = mean(cross_val_score(model_ada, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['ExtraTrees'] = mean(cross_val_score(model_ext, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['RandomForest'] = mean(cross_val_score(model_rf, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['HistGradient'] = mean(cross_val_score(model_hgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['Voting'] = mean(cross_val_score(model_vot, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['GradBoost'] = mean(cross_val_score(model_gb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        fit['NeuralN'] = mean(cross_val_score(model_nn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "        \n",
    "        # XGBoost needs to LabelEncode the y before fitting the model\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder().fit(y)\n",
    "        y_encoded = le.transform(y.copy())\n",
    "        fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y_encoded, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "      \n",
    "        # Add the model to another dictionary; make sure the keys have the same names as the list above\n",
    "        models['Logistic'] = model_log\n",
    "        models['Ridge'] = model_logcv\n",
    "        models['SGD'] = model_sgd\n",
    "        models['PassiveAggressive'] = model_pa\n",
    "        models['Perceptron'] = model_per\n",
    "        models['KNN'] = model_knn\n",
    "        models['SVM'] = model_svm\n",
    "        models['NaiveBayes'] = model_nb\n",
    "        models['Bagging'] = model_bag\n",
    "        models['AdaBoost'] = model_ada\n",
    "        models['ExtraTrees'] = model_ext\n",
    "        models['RandomForest'] = model_rf\n",
    "        models['HistGradient'] = model_hgb\n",
    "        models['Voting'] = model_vot\n",
    "        models['GradBoost'] = model_gb\n",
    "        models['XGBoost'] = model_xgb\n",
    "        models['NeuralN'] = model_nn\n",
    "      \n",
    "        # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "        df_fit = pd.DataFrame({'Accuracy':fit})\n",
    "        df_fit.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
    "        best_model = df_fit.index[0]\n",
    "        print(df_fit)\n",
    "      \n",
    "        return models[best_model].fit(X, y)\n",
    "\n",
    "model = fit_cv_classification_expanded(df, 'home_win')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
